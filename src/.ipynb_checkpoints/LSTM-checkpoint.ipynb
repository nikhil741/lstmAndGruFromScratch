{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\"> LSTM RNN ON 28*28 MNIST DATASET TO PREDICT TEN CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color:blue\">Its a dynamic sequence and batch LSTM . This is created with tensorflow scan and map higher ops!!!! \n",
    "###  <span style=\"color:blue\">This is a base LSTM which can be used to create Neural Stack Machine, Neural Turing Machine and  RNN-EM and so on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "import sys\n",
    "from data_loader import DataLoader\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "if not os.path.exists(\"../weights\"):\n",
    "    os.mkdir(\"../weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM class and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_cell(object):\n",
    "\n",
    "    \"\"\"\n",
    "    LSTM cell object which takes 3 arguments for initialization.\n",
    "    input_size = Input Vector size\n",
    "    hidden_layer_size = Hidden layer size\n",
    "    target_size = Output vector size\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_layer_size, target_size):\n",
    "\n",
    "        # Initialization of given values\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.target_size = target_size\n",
    "\n",
    "        # Weights and Bias for input and hidden tensor\n",
    "        self.Wi = tf.Variable(tf.zeros(\n",
    "            [self.input_size, self.hidden_layer_size]))\n",
    "        self.Ui = tf.Variable(tf.zeros(\n",
    "            [self.hidden_layer_size, self.hidden_layer_size]))\n",
    "        self.bi = tf.Variable(tf.zeros([self.hidden_layer_size]))\n",
    "\n",
    "        \n",
    "        self.Wf = tf.Variable(tf.zeros(\n",
    "            [self.input_size, self.hidden_layer_size]))\n",
    "        self.Uf = tf.Variable(tf.zeros(\n",
    "            [self.hidden_layer_size, self.hidden_layer_size]))\n",
    "        self.bf = tf.Variable(tf.zeros([self.hidden_layer_size]))        \n",
    "        \n",
    "        \n",
    "        self.Wog = tf.Variable(tf.zeros(\n",
    "            [self.input_size, self.hidden_layer_size]))\n",
    "        self.Uog = tf.Variable(tf.zeros(\n",
    "            [self.hidden_layer_size, self.hidden_layer_size]))\n",
    "        self.bog = tf.Variable(tf.zeros([self.hidden_layer_size]))        \n",
    "        \n",
    "        \n",
    "        self.Wc = tf.Variable(tf.zeros(\n",
    "            [self.input_size, self.hidden_layer_size]))\n",
    "        self.Uc = tf.Variable(tf.zeros(\n",
    "            [self.hidden_layer_size, self.hidden_layer_size]))\n",
    "        self.bc = tf.Variable(tf.zeros([self.hidden_layer_size]))        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Weights for output layers\n",
    "        self.Wo = tf.Variable(tf.truncated_normal(\n",
    "            [self.hidden_layer_size, self.target_size],mean=0,stddev=.01))\n",
    "        self.bo = tf.Variable(tf.truncated_normal([self.target_size],mean=0,stddev=.01))\n",
    "\n",
    "        # Placeholder for input vector with shape[batch, seq, embeddings]\n",
    "        self._inputs = tf.placeholder(tf.float32,\n",
    "                                      shape=[None, None, self.input_size],\n",
    "                                      name='inputs')\n",
    "\n",
    "        # Processing inputs to work with scan function\n",
    "        self.processed_input = process_batch_input_for_RNN(self._inputs)\n",
    "\n",
    "        '''\n",
    "        Initial hidden state's shape is [1,self.hidden_layer_size]\n",
    "        In First time stamp, we are doing dot product with weights to\n",
    "        get the shape of [batch_size, self.hidden_layer_size].\n",
    "        For this dot product tensorflow use broadcasting. But during\n",
    "        Back propagation a low level error occurs.\n",
    "        So to solve the problem it was needed to initialize initial\n",
    "        hiddden state of size [batch_size, self.hidden_layer_size].\n",
    "        So here is a little hack !!!! Getting the same shaped\n",
    "        initial hidden state of zeros.\n",
    "        '''\n",
    "\n",
    "        self.initial_hidden = self._inputs[:, 0, :]\n",
    "        self.initial_hidden= tf.matmul(\n",
    "            self.initial_hidden, tf.zeros([input_size, hidden_layer_size]))\n",
    "        \n",
    "        \n",
    "        self.initial_hidden=tf.stack([self.initial_hidden,self.initial_hidden])\n",
    "    # Function for LSTM cell.\n",
    "    def Lstm(self, previous_hidden_memory_tuple, x):\n",
    "        \"\"\"\n",
    "        This function takes previous hidden state and memory tuple with input and\n",
    "        outputs current hidden state.\n",
    "        \"\"\"\n",
    "        \n",
    "        previous_hidden_state,c_prev=tf.unstack(previous_hidden_memory_tuple)\n",
    "        \n",
    "        #Input Gate\n",
    "        i= tf.sigmoid(\n",
    "            tf.matmul(x,self.Wi)+tf.matmul(previous_hidden_state,self.Ui) + self.bi \n",
    "        )\n",
    "        \n",
    "        #Forget Gate\n",
    "        f= tf.sigmoid(\n",
    "            tf.matmul(x,self.Wf)+tf.matmul(previous_hidden_state,self.Uf) + self.bf \n",
    "        )\n",
    "        \n",
    "        #Output Gate\n",
    "        o= tf.sigmoid(\n",
    "            tf.matmul(x,self.Wog)+tf.matmul(previous_hidden_state,self.Uog) + self.bog\n",
    "        )\n",
    "        \n",
    "        #New Memory Cell\n",
    "        c_= tf.nn.tanh(\n",
    "            tf.matmul(x,self.Wc)+tf.matmul(previous_hidden_state,self.Uc) + self.bc \n",
    "        ) \n",
    "        \n",
    "        #Final Memory cell\n",
    "        c= f*c_prev + i*c_\n",
    "        \n",
    "        #Current Hidden state\n",
    "        current_hidden_state = o*tf.nn.tanh(c)\n",
    "\n",
    "\n",
    "        return tf.stack([current_hidden_state,c])\n",
    "\n",
    "    # Function for getting all hidden state.\n",
    "    def get_states(self):\n",
    "        \"\"\"\n",
    "        Iterates through time/ sequence to get all hidden state\n",
    "        \"\"\"\n",
    "\n",
    "        # Getting all hidden state throuh time\n",
    "        all_hidden_states = tf.scan(self.Lstm,\n",
    "                                    self.processed_input,\n",
    "                                    initializer=self.initial_hidden,\n",
    "                                    name='states')\n",
    "        all_hidden_states=all_hidden_states[:,0,:,:]\n",
    "        \n",
    "        return all_hidden_states\n",
    "\n",
    "    # Function to get output from a hidden layer\n",
    "    def get_output(self, hidden_state):\n",
    "        \"\"\"\n",
    "        This function takes hidden state and returns output\n",
    "        \"\"\"\n",
    "        output = tf.nn.relu(tf.matmul(hidden_state, self.Wo) + self.bo)\n",
    "\n",
    "        return output\n",
    "\n",
    "    # Function for getting all output layers\n",
    "    def get_outputs(self):\n",
    "        \"\"\"\n",
    "        Iterating through hidden states to get outputs for all timestamp\n",
    "        \"\"\"\n",
    "        all_hidden_states = self.get_states()\n",
    "\n",
    "        all_outputs = tf.map_fn(self.get_output, all_hidden_states)\n",
    "\n",
    "        return all_outputs\n",
    "\n",
    "\n",
    "# Function to convert batch input data to use scan ops of tensorflow.\n",
    "def process_batch_input_for_RNN(batch_input):\n",
    "    \"\"\"\n",
    "    Process tensor of size [5,3,2] to [3,5,2]\n",
    "    \"\"\"\n",
    "    batch_input_ = tf.transpose(batch_input, perm=[2, 0, 1])\n",
    "    X = tf.transpose(batch_input_)\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholder and initializers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 30\n",
    "input_size = 28\n",
    "target_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float32, shape=[None, target_size],name='inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing rnn object\n",
    "rnn=LSTM_cell( input_size, hidden_layer_size, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting all outputs from rnn\n",
    "outputs = rnn.get_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting final output through indexing after reversing\n",
    "#last_output = tf.reverse(outputs,[True,False,False])[0,:,:]\n",
    "last_output = outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As rnn model output the final layer through Relu activation softmax is used for final output.\n",
    "output=tf.nn.softmax(last_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the Cross Entropy loss \n",
    "cross_entropy = -tf.reduce_sum(y * tf.log(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainning with Adadelta Optimizer\n",
    "train_step = tf.train.AdamOptimizer().minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculatio of correct prediction and accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(output,1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get on hot\n",
    "def get_on_hot(number):\n",
    "    on_hot=[0]*10\n",
    "    on_hot[number]=1\n",
    "    return on_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "dataLoaderObject = DataLoader()\n",
    "#Using Sklearn MNIST dataset.\n",
    "#digits = datasets.load_digits()\n",
    "#X=digits.images\n",
    "#Y_=digits.target\n",
    "X_train, Y_ = dataLoaderObject.load_data()\n",
    "X_train = X_train.reshape(60000, 28, 28)\n",
    "X_test, y_test = dataLoaderObject.load_data(mode='test')\n",
    "#X_test = X_test.reshape()\n",
    "y_train=map(get_on_hot,Y_)\n",
    "y_test = map(get_on_hot, y_test)\n",
    "X_test = X_test.reshape(10000, 28, 28)\n",
    "print X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Train and test Dataset\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.22, random_state=42)\n",
    "\n",
    "#Cuttting for simple iteration\n",
    "X_train=X_train[:]\n",
    "y_train=y_train[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nikhil741/.local/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "#----(SAVER OBJECT) TO SAVE WEIGHT PAREMETER TO HARD-DISK-----#\n",
    "saver = tf.train.Saver()\n",
    "sess=tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAD8CAYAAAC2PJlnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efe7dd918d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Loss: 71.1757 Train Accuracy: 72.8 Test Accuracy: 73.19 Saved_Parameters: ../weights/model.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAD8CAYAAAC2PJlnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efe7dd918d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Iterations to do trainning\n",
    "for epoch in range(1):\n",
    "    \n",
    "    start=0\n",
    "    end=100\n",
    "    for i in range(600):\n",
    "        \n",
    "        X=X_train[start:end]\n",
    "        Y=y_train[start:end]\n",
    "        start=end\n",
    "        end=start+100\n",
    "        #print X.shape, len(Y)\n",
    "        sess.run(train_step,feed_dict={rnn._inputs:X, y:Y})\n",
    "    \n",
    "    Loss=str(sess.run(cross_entropy,feed_dict={rnn._inputs:X, y:Y}))\n",
    "    Train_accuracy=str(sess.run(accuracy,feed_dict={rnn._inputs:X_train[:500], y:y_train[:500]}))\n",
    "    Test_accuracy=str(sess.run(accuracy,feed_dict={rnn._inputs:X_test, y:y_test}))\n",
    "    \n",
    "\n",
    "    pl.plot([epoch],Loss,'b.',)\n",
    "    pl.plot([epoch],Train_accuracy,'r*',)\n",
    "    pl.plot([epoch],Test_accuracy,'g+')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(pl.gcf())   \n",
    "    \n",
    "    sys.stdout.flush()\n",
    "    print(\"\\rIteration: %s Loss: %s Train Accuracy: %s Test Accuracy: %s\"%(epoch,Loss,Train_accuracy,Test_accuracy)),\n",
    "    sys.stdout.flush()\n",
    "\n",
    "#---------------------SAVE PARAMETERS TO DISK----------------------------------------------------------------#\n",
    "save_path = saver.save(sess, \"../weights/model.ckpt\")\n",
    "print \"Saved_Parameters:\", save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../weights/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\t        new_saver = tf.train.import_meta_graph('../weights/model.ckpt.meta')\n",
    "\t        new_saver.restore(sess, tf.train.latest_checkpoint('../weights'))\n",
    "            sess.run(train_step,feed_dict={rnn._inputs:X, y:Y})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'tag_constants' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-88cbd2de09c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mload_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-ef4b90c1f275>\u001b[0m in \u001b[0;36mload_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtag_constants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAINING\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: global name 'tag_constants' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
